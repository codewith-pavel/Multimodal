{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVmYbqqUSlCH"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Fine-tuning Florence-2 on Object Detection Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/florence-2/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)\n",
        "\n",
        "Florence-2 is a lightweight vision-language model open-sourced by Microsoft under the MIT license. The model demonstrates strong zero-shot and fine-tuning capabilities across tasks such as captioning, object detection, grounding, and segmentation.\n",
        "\n",
        "![Florence-2 Figure.1](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-1.png)\n",
        "\n",
        "*Figure 1. Illustration showing the level of spatial hierarchy and semantic granularity expressed by each task. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n",
        "The model takes images and task prompts as input, generating the desired results in text format. It uses a DaViT vision encoder to convert images into visual token embeddings. These are then concatenated with BERT-generated text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response.\n",
        "\n",
        "![Florence-2 Figure.2](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/florence-2-figure-2.png)\n",
        "\n",
        "*Figure 2. Overview of Florence-2 architecture. Source: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z16cfHRE8yi8"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqd30Ndg9dbt"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune Florence-2, you need to provide your HuggingFace Token and Roboflow API key. Follow these steps:\n",
        "\n",
        "- Open your [`HuggingFace Settings`](https://huggingface.co/settings) page. Click `Access Tokens` then `New Token` to generate new token.\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (üîë).\n",
        "    - Store HuggingFace Access Token under the name `HF_TOKEN`.\n",
        "    - Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n32nrwCeAEYP"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `L4 GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMmBuhiiC2mX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOshHQM3Ebq5"
      },
      "source": [
        "### Download example data\n",
        "\n",
        "**NOTE:** Feel free to replace our example image with your own photo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3ZhBCTPEnEO"
      },
      "outputs": [],
      "source": [
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!ls -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbglpBOOFCHm"
      },
      "outputs": [],
      "source": [
        "EXAMPLE_IMAGE_PATH = \"dog.jpeg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4QlaUfCFsv"
      },
      "source": [
        "## Download and configure the model\n",
        "\n",
        " Let's download the model checkpoint and configure it so that you can fine-tune it later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6b1dvjgYXOD"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers flash_attn timm einops peft\n",
        "!pip install -q roboflow git+https://github.com/roboflow/supervision.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMd6tb4sSh9G"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import html\n",
        "import base64\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from google.colab import userdata\n",
        "from IPython.core.display import display, HTML\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoProcessor,\n",
        "    get_scheduler\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any, Tuple, Generator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "from roboflow import Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flp13B-8Myjf"
      },
      "source": [
        "Load the model using `AutoModelForCausalLM` and the processor using `AutoProcessor` classes from the transformers library. Note that you need to pass `trust_remote_code` as `True` since this model is not a standard transformers model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqDWEWDcaSxN"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
        "REVISION = 'refs/pr/6'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION).to(DEVICE)\n",
        "processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True, revision=REVISION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf1GlvvQFec-"
      },
      "source": [
        "## Run inference with pre-trained Florence-2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReAKWNxAFmv1"
      },
      "outputs": [],
      "source": [
        "# @title Example object detection inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSLSErXeI84L"
      },
      "outputs": [],
      "source": [
        "# @title Example image captioning inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<DETAILED_CAPTION>\"\n",
        "text = \"<DETAILED_CAPTION>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrVSTmFPJjR6"
      },
      "outputs": [],
      "source": [
        "# @title Example caption to phrase grounding inference\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
        "text = \"<CAPTION_TO_PHRASE_GROUNDING> In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQetrQM7Jziy"
      },
      "source": [
        "## Fine-tune Florence-2 on custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7D6ZYzAs9a"
      },
      "source": [
        "### Download dataset from Roboflow Universe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1IlyjYmBCxX"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"aOSlCDm0nLxDCPNCt8R6\")\n",
        "project = rf.workspace(\"p-aqwkg\").project(\"tooth-endodontics-treatment\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"florence2-od\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiLclUnKTrLE"
      },
      "outputs": [],
      "source": [
        "!head -n 5 {dataset.location}/train/annotations.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dExvJNFkxymc"
      },
      "outputs": [],
      "source": [
        "# @title Define `DetectionsDataset` class\n",
        "\n",
        "class JSONLDataset:\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.jsonl_file_path = jsonl_file_path\n",
        "        self.image_directory_path = image_directory_path\n",
        "        self.entries = self._load_entries()\n",
        "\n",
        "    def _load_entries(self) -> List[Dict[str, Any]]:\n",
        "        entries = []\n",
        "        with open(self.jsonl_file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data = json.loads(line)\n",
        "                entries.append(data)\n",
        "        return entries\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:\n",
        "        if idx < 0 or idx >= len(self.entries):\n",
        "            raise IndexError(\"Index out of range\")\n",
        "\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_path, entry['image'])\n",
        "        try:\n",
        "            image = Image.open(image_path)\n",
        "            return (image, entry)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Image file {image_path} not found.\")\n",
        "\n",
        "\n",
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_path: str):\n",
        "        self.dataset = JSONLDataset(jsonl_file_path, image_directory_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, data = self.dataset[idx]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        return prefix, suffix, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilMb0ivGdt9l"
      },
      "outputs": [],
      "source": [
        "# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\n",
        "\n",
        "BATCH_SIZE = 6\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "def collate_fn(batch):\n",
        "    questions, answers, images = zip(*batch)\n",
        "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    return inputs, answers\n",
        "\n",
        "train_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/train/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/train/\"\n",
        ")\n",
        "val_dataset = DetectionDataset(\n",
        "    jsonl_file_path = f\"{dataset.location}/valid/annotations.jsonl\",\n",
        "    image_directory_path = f\"{dataset.location}/valid/\"\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmPJOXCzB-29"
      },
      "outputs": [],
      "source": [
        "# @title Setup LoRA Florence-2 model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    inference_mode=False,\n",
        "    use_rslora=True,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    revision=REVISION\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V9BcVQMycgq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9LEEXRwN9cX"
      },
      "outputs": [],
      "source": [
        "# @title Run inference with pre-trained Florence-2 model on validation dataset\n",
        "\n",
        "def render_inline(image: Image.Image, resize=(128, 128)):\n",
        "    \"\"\"Convert image into inline html.\"\"\"\n",
        "    image.resize(resize)\n",
        "    with io.BytesIO() as buffer:\n",
        "        image.save(buffer, format='jpeg')\n",
        "        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
        "        return f\"data:image/jpeg;base64,{image_b64}\"\n",
        "\n",
        "\n",
        "def render_example(image: Image.Image, response):\n",
        "    try:\n",
        "        detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "        image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n",
        "        image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n",
        "    except:\n",
        "        print('failed to redner model response')\n",
        "    return f\"\"\"\n",
        "<div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
        "    <img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" />\n",
        "    <p style=\"width:512px; margin:10px; font-size:small;\">{html.escape(json.dumps(response))}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def render_inference_results(model, dataset: DetectionDataset, count: int):\n",
        "    html_out = \"\"\n",
        "    count = min(count, len(dataset))\n",
        "    for i in range(count):\n",
        "        image, data = dataset.dataset[i]\n",
        "        prefix = data['prefix']\n",
        "        suffix = data['suffix']\n",
        "        inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3\n",
        "        )\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "        answer = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "        html_out += render_example(image, answer)\n",
        "\n",
        "    display(HTML(html_out))\n",
        "\n",
        "render_inference_results(peft_model, val_dataset, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH9JTq_RytE2"
      },
      "source": [
        "## Fine-tune Florence-2 on custom object detection dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC06Mc7jOdpY"
      },
      "outputs": [],
      "source": [
        "# @title Define train loop\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(train_loader, val_loader, model, processor, epochs=10, lr=1e-6):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    num_training_steps = epochs * len(train_loader)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "\n",
        "    render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "    # Initialize GradScaler for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            pixel_values = inputs[\"pixel_values\"]\n",
        "            labels = processor.tokenizer(\n",
        "                text=answers,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                return_token_type_ids=False\n",
        "            ).input_ids.to(DEVICE)\n",
        "\n",
        "            # Mixed precision training\n",
        "            with autocast():\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Clear GPU cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
        "                input_ids = inputs[\"input_ids\"]\n",
        "                pixel_values = inputs[\"pixel_values\"]\n",
        "                labels = processor.tokenizer(\n",
        "                    text=answers,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    return_token_type_ids=False\n",
        "                ).input_ids.to(DEVICE)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "            render_inference_results(peft_model, val_loader.dataset, 6)\n",
        "\n",
        "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        model.save_pretrained(output_dir)\n",
        "        processor.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZybGHd3fNJ1"
      },
      "outputs": [],
      "source": [
        "# @title Run train loop\n",
        "\n",
        "%%time\n",
        "\n",
        "EPOCHS = 100\n",
        "LR = 5e-6\n",
        "\n",
        "train_model(train_loader, val_loader, peft_model, processor, epochs=EPOCHS, lr=LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBHMu7WGWpeu"
      },
      "source": [
        "## Fine-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xspOKfgRY2gX"
      },
      "outputs": [],
      "source": [
        "# @title Check if the model can still detect objects outside of the custom dataset\n",
        "\n",
        "image = Image.open(EXAMPLE_IMAGE_PATH)\n",
        "task = \"<OD>\"\n",
        "text = \"<OD>\"\n",
        "\n",
        "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "generated_ids = peft_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    pixel_values=inputs[\"pixel_values\"],\n",
        "    max_new_tokens=1024,\n",
        "    num_beams=3\n",
        ")\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "response = processor.post_process_generation(generated_text, task=task, image_size=(image.width, image.height))\n",
        "detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n",
        "\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "label_annotator = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
        "\n",
        "image = bounding_box_annotator.annotate(image, detections)\n",
        "image = label_annotator.annotate(image, detections)\n",
        "image.thumbnail((600, 600))\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ygh9VyZaqnb"
      },
      "source": [
        "**NOTE:** It seems that the model can still detect classes that don't belong to our custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f1BYeQw3xhl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Define the regular expression pattern to extract classes\n",
        "PATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)<loc_\\d+>'\n",
        "\n",
        "# Function to extract unique classes from the dataset\n",
        "def extract_classes(dataset: DetectionDataset):\n",
        "    class_set = set()\n",
        "    for i in range(len(dataset.dataset)):\n",
        "        image, data = dataset.dataset[i]\n",
        "        suffix = data[\"suffix\"]\n",
        "        classes = re.findall(PATTERN, suffix)\n",
        "        class_set.update(classes)\n",
        "    return sorted(class_set)\n",
        "\n",
        "# Extract classes from the training dataset\n",
        "CLASSES = extract_classes(train_dataset)\n",
        "\n",
        "# Initialize lists to store targets and predictions\n",
        "targets = []\n",
        "predictions = []\n",
        "\n",
        "# Loop through the validation dataset\n",
        "for i in range(len(val_dataset.dataset)):\n",
        "    image, data = val_dataset.dataset[i]\n",
        "    prefix = data['prefix']\n",
        "    suffix = data['suffix']\n",
        "\n",
        "    # Process the input and generate predictions\n",
        "    inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        pixel_values=inputs[\"pixel_values\"],\n",
        "        max_new_tokens=1024,\n",
        "        num_beams=3\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    # Post-process the generated text to extract predictions\n",
        "    prediction = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n",
        "    prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n",
        "\n",
        "    # Filter predictions to only include known classes\n",
        "    prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n",
        "\n",
        "    # Assign class IDs based on known classes\n",
        "    prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n",
        "    prediction.confidence = np.ones(len(prediction))\n",
        "\n",
        "    # Post-process the target annotations\n",
        "    target = processor.post_process_generation(suffix, task='<OD>', image_size=image.size)\n",
        "    target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n",
        "\n",
        "    # Handle cases where target class names are not in CLASSES\n",
        "    valid_class_names = []\n",
        "    for class_name in target['class_name']:\n",
        "        if class_name not in CLASSES:\n",
        "            print(f\"Warning: '{class_name}' not found in CLASSES. Adding it.\")\n",
        "            CLASSES.append(class_name)  # Add missing class to CLASSES dynamically\n",
        "        valid_class_names.append(class_name)\n",
        "\n",
        "    # Assign class IDs for the target based on updated CLASSES\n",
        "    target.class_id = np.array([CLASSES.index(class_name) for class_name in valid_class_names])\n",
        "\n",
        "    # Append the processed target and prediction to the lists\n",
        "    targets.append(target)\n",
        "    predictions.append(prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88VnIZ_feHPo"
      },
      "outputs": [],
      "source": [
        "# @title Calculate mAP\n",
        "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        ")\n",
        "\n",
        "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
        "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
        "print(f\"map75: {mean_average_precision.map75:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85APzNRfe8xp"
      },
      "outputs": [],
      "source": [
        "# @title Calculate Confusion Matrix\n",
        "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
        "    predictions=predictions,\n",
        "    targets=targets,\n",
        "    classes=CLASSES\n",
        ")\n",
        "\n",
        "_ = confusion_matrix.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rR2naNXzEB0"
      },
      "source": [
        "## Save fine-tuned model on hard drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdbmcv3TcIe8"
      },
      "outputs": [],
      "source": [
        "peft_model.save_pretrained(\"/content/florence2-lora\")\n",
        "processor.save_pretrained(\"/content/florence2-lora/\")\n",
        "!ls -la /content/florence2-lora/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag0XROk7fcd_"
      },
      "source": [
        "# Congratulations\n",
        "\n",
        "‚≠êÔ∏è If you enjoyed this notebook, [**star the Roboflow Notebooks repo**](https://https://github.com/roboflow/notebooks) (and [**supervision**](https://github.com/roboflow/supervision) while you're at it) and let us know what tutorials you'd like to see us do next. ‚≠êÔ∏è"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}